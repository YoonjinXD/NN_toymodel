{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bit of setup\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "    \"\"\"\n",
    "    two-layer-perceptron.\n",
    "    Input dimension : N\n",
    "    Hidden layer dimension : H\n",
    "    Output dimension : C\n",
    "\n",
    "    input - linear layer - ReLU - linear layer - output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, std=1e-4):\n",
    "        \"\"\"\n",
    "        W1: first layer's weight; (D, H)\n",
    "        b1: first layer's bias; (H,)\n",
    "        W2: second layer's weight; (H, C)\n",
    "        b2: second layer's bias; (C,)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params['W1'] = std * torch.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = torch.zeros(hidden_size)\n",
    "        self.params['W2'] = std * torch.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = torch.zeros(output_size)\n",
    "    \n",
    "\n",
    "    def loss(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Neural network의 loss와 gradient를 직접 계산하자.\n",
    "\n",
    "        Inputs:\n",
    "        - X: Input data. shape (N, D). 각각의 X[i]가 하나의 training sample이며 총 N개의 sample이 input으로 주어짐.\n",
    "        - y: Training label 벡터. y[i]는 X[i]에 대한 정수값의 label.\n",
    "          y가 주어질 경우 loss와 gradient를 반환하며 y가 주어지지 않으면 output을 반환\n",
    "\n",
    "        Returns:\n",
    "        y가 주어지지 않으면, shape (N, C)인 score matrix 반환\n",
    "        scores[i, c]는 input X[i]에 대한 class c의 score\n",
    "\n",
    "        y가 주어지면 (loss, grads) tuple 반환\n",
    "        loss: training batch에 대한 loss (scalar)\n",
    "        grads: {parameter 이름: gradient} 형태의 dictionary\n",
    "        \"\"\"\n",
    "        W1, b1 = self.params['W1'], self.params['b1']\n",
    "        W2, b2 = self.params['W2'], self.params['b2']\n",
    "        N, D = X.size()\n",
    "\n",
    "        # Forward path\n",
    "        hidden = X.mm(W1) + b1\n",
    "        hidden_relu = hidden.clamp(min=0)\n",
    "        scores = hidden_relu.mm(W2) + b2\n",
    "\n",
    "        # 정답(target)이 주어지지 않은 경우 점수를 리턴하고 종료\n",
    "        if y is None:\n",
    "            return scores\n",
    "\n",
    "        # Softmax loss function\n",
    "        e = torch.exp(scores)\n",
    "        softmax = e / torch.sum(e, dim=1, keepdim=True)\n",
    "        ### 이부분 더 쉽게 바꿀 수 있지 않을까 ###\n",
    "        N, C = softmax.size()\n",
    "        y_tensor = torch.zeros(N, C)\n",
    "        for i in range(0, N):\n",
    "            y_tensor[i, y[i]] = 1\n",
    "        nll = -1 * torch.log(softmax)\n",
    "        loss = (nll.reshape(1, N*C)).mm(y_tensor.reshape(N*C, 1))\n",
    "\n",
    "        # Backward path(Gradient calculation)\n",
    "        grads = {} \n",
    "        ### 이부분 cs231n 코드에 dictionary 활용부분 봐야됨 ###\n",
    "        softmax_grads = softmax.sub(y_tensor)\n",
    "        grads['W2'] = (hidden_relu.t()).mm(softmax_grads)\n",
    "        grads['b2'] = softmax_grads\n",
    "        hidden_relu_grads = softmax_grads.mm(W2.t())\n",
    "        hidden_grads = hidden_relu_grads.clone()\n",
    "        grads['W1'] = (X.t()).mm(hidden_grads)\n",
    "        grads['b1'] = hidden_grads\n",
    "\n",
    "        return loss, grads\n",
    "\n",
    "    def train(self, X, y,\n",
    "            learning_rate=1e-3, learning_rate_decay=0.95,\n",
    "            num_iters=100,\n",
    "            batch_size=200, verbose=False):\n",
    "        \"\"\"\n",
    "        SGD training\n",
    "\n",
    "        Inputs:\n",
    "        - X: shape (N, D)의 numpy array (training data)\n",
    "        - y: shape (N,)의 numpy array(training labels; y[i] = c\n",
    "                                      c는 X[i]의 label, 0 <= c < C)\n",
    "        - learning_rate: Scalar learning rate\n",
    "        - num_iters: Number of steps\n",
    "        - batch_size: Number of training examples in a mini-batch.\n",
    "        - verbose: true일 경우 progress 출력\n",
    "        \"\"\"\n",
    "        num_train = X.shape[0]\n",
    "        iterations_per_epoch = max(num_train / batch_size, 1)\n",
    "\n",
    "        # SGD optimization\n",
    "        loss_history = []\n",
    "        train_acc_history = []\n",
    "        val_acc_history = []\n",
    "\n",
    "        for it in range(num_iters):\n",
    "            loss, grads = self.loss(X, y=y)\n",
    "            loss_history.append(loss)\n",
    "            self.params['W1'] = self.params['W1'] - learning_rate * grads['W1']\n",
    "            self.params['W2'] = self.params['W2'] - learning_rate * grads['W2']\n",
    "            self.params['b1'] = self.params['b1'] - learning_rate * grads['b1']\n",
    "            self.params['b2'] = self.params['b2'] - learning_rate * grads['b2']\n",
    "\n",
    "            if verbose and it % 100 == 0:\n",
    "                print('iteration %d / %d: loss %f' % (it, num_iters, loss))\n",
    "\n",
    "            if it % iterations_per_epoch == 0:\n",
    "                # Accuracy\n",
    "                train_acc = (self.predict(X) == y).float().mean()\n",
    "                train_acc_history.append(train_acc)\n",
    "                learning_rate *= learning_rate_decay\n",
    "\n",
    "        return {\n",
    "          'loss_history': loss_history,\n",
    "          'train_acc_history': train_acc_history,\n",
    "          'val_acc_history': val_acc_history,\n",
    "        }\n",
    "\n",
    "    def predict(self, X):\n",
    "        return torch.argmax(self.loss(X),1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a network model\n",
      "Created a test data\n"
     ]
    }
   ],
   "source": [
    "input_size = 4\n",
    "hidden_size = 10\n",
    "num_classes = 3\n",
    "num_inputs = 5\n",
    "\n",
    "def init_toy_model():\n",
    "    torch.manual_seed(0)\n",
    "    print(\"Created a network model\")\n",
    "    return TwoLayerNet(input_size, hidden_size, num_classes, std=1e-1)\n",
    "\n",
    "def init_toy_data():\n",
    "    torch.manual_seed(1)\n",
    "    X = 10 * torch.randn(num_inputs, input_size)\n",
    "    y = torch.LongTensor([0, 1, 2, 2, 1])\n",
    "    print(\"Created a test data\")\n",
    "    return X, y\n",
    "\n",
    "net = init_toy_model()\n",
    "X, y = init_toy_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your scores:\n",
      "tensor([[ 0.2462,  0.1262,  1.1628],\n",
      "        [ 0.1836, -0.0676, -0.2131],\n",
      "        [-0.2075, -0.1253, -0.0651],\n",
      "        [ 0.0864,  0.0717,  0.2353],\n",
      "        [ 0.8220, -0.3256, -0.7781]])\n",
      "\n",
      "correct scores:\n",
      "tensor([[ 0.2462,  0.1262,  1.1628],\n",
      "        [ 0.1836, -0.0676, -0.2131],\n",
      "        [-0.2075, -0.1253, -0.0651],\n",
      "        [ 0.0864,  0.0717,  0.2353],\n",
      "        [ 0.8220, -0.3256, -0.7781]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "scores = net.loss(X)\n",
    "print('Your scores:')\n",
    "print(scores)\n",
    "print()\n",
    "print('correct scores:')\n",
    "correct_scores = torch.Tensor(\n",
    "  [[ 0.24617445,  0.1261572,   1.1627575 ],\n",
    " [ 0.18364899, -0.0675799,  -0.21310908],\n",
    " [-0.2075074,  -0.12525336, -0.06508598],\n",
    " [ 0.08643292,  0.07172455,  0.2353122 ],\n",
    " [ 0.8219606,  -0.32560882, -0.77807254]]\n",
    ")\n",
    "print(correct_scores)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference between your scores and correct scores:\n",
      "tensor(1.5646e-07)\n",
      "Difference between your loss and correct loss:\n",
      "tensor(4.9777)\n"
     ]
    }
   ],
   "source": [
    "print('Difference between your scores and correct scores:')\n",
    "print(torch.sum(torch.abs(scores - correct_scores)))\n",
    "\n",
    "loss, _ = net.loss(X, y)\n",
    "correct_loss = 1.2444149\n",
    "\n",
    "print('Difference between your loss and correct loss:')\n",
    "print(torch.sum(torch.abs(loss - correct_loss)))\n",
    "\n",
    "loss, grads = net.loss(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.400000 -> 1.000000\n",
      "Train loss: 6.222075 -> 0.159872\n"
     ]
    }
   ],
   "source": [
    "results = net.train(X, y, 0.05)\n",
    "print(\"Train acc: %f -> %f\\nTrain loss: %f -> %f\" % (results['train_acc_history'][0], results['train_acc_history'][-1]\n",
    "                                                , results['loss_history'][0],results['loss_history'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
